# 20190624

월화수, AI,Bigdata, blockchain-오전 elearning, 오후 수업

## 1장 Numpy 다루기

### AI/ML 소개 강의 영상

#### ML

* vs Big data- 데이터가 많다. 분석하는 방법론중 가장 핫한게 ML
* vs 데이터 마이닝- 데이터 자체 차이, 정형 데이터를 쓴다. ML은 비정형을 주로 쓴다.
* vs AI- 머신러닝과 동일처럼 쓰지만 달라, 사람의 지능을 흉내내는 AI, 지능을 흉내내는 방법중 데이터를 이용한 방법이 ML
* vs Statistics(통계학)- 통계학자들이 증명한 많은 것 을 가져다 쓴다. 프라이벤 한계를 돌파한다.

#### ML이 쓰는 방법론

* 지도학습
  * 누군가가 지도 해준다. 정답이 있는 데이터로 학습
  * 새로운 데이터로 실험
  * 선형/비선형 모델
    * 비선형 모델 - 데이터에 질문을 계속 하는 모델
    * feature를 뽑아 낸다.
    * 결정 트리를 만들어서 예측한다.
    * 목적은 똑같다. 특정 데이터에 대해 분류 하는 것,
    * 선형이냐 비선형이냐
* 비지도 학습
  * 정답이 있는 데이터를 구하기 힘들다면 비지도 학습을 주로 쓴다.
  * 사람이 주로 쓰는 방법
  * 비슷한 것끼리 카테고리를 나눠 간다.
  * 카테고리 분류를 어떻게 할것인가? 몇 그룹으로 나눌 것인가? 스스로 나누는 것마다 다르다.
  * 기준 중점 찾고 나누는게 좋은가? 뭉쳐져 있는 것 묶어야지
  * 임으로 잡아서 옆에 것들 묶는 방식
* 강화학습
  * 크게 안함(어려움)
* representation learning(딥러닝이라 생각 가능)
  * 무엇인가? 왜 중요한가?
  * 2000년대 넘어가며 data가 엄청 커졌다.
  * 자동차, 고양이 분류 등은 데이터 큰게 꼭 도움 되지 만은 않는다.
  * 얼굴인식
    * 얼굴의 경계 찾는 것
    * 첫번째layer: 픽셀 하나하나 보기
    * 두번째layer: 픽셀을 엮어서 대조 되서 선이 되는 것을 찾는다.
    * 세번째 layer: 선과 곡선의 조합으로 눈과 코 찾는다.
    * 네번째 layser: 얼굴형
    * 사친 연산으로 눈, 코, 입 찾고 얼굴 찾아
    * 과거엔 선찾기, 복사 찾기 알고리즘 다 만들어서 썼다.
  * 왜 지금각광 받는가?
    * 1950년대부터 썼다
    * 모델의 표현력이 풍부-> 계산 오래 걸려, 데이터도 많아
    * 2000년도 되면서 컴퓨터 성능 상승, data 확장
    * 지나치게 적화 되면 오버피팅을 드랍아웃으로 해결- 앤드루 응

#### 유명 AI

* 딥블루 - 체스 마스터 이긴 AI,1997
* 자율주행 - 연구 오래 돼, 2007년 Urban 챌린지에서 많이 바껴
* IBM의 왓슨 - 제퍼디 퀴즈쇼 우승 - 2011
* 알파고 - 이세돌, 2016
* 알파고 제로 - 학습 데이터 없이 학습, 스스로 시뮬레이션, 3일 만에 이세돌 이긴 수준, 21일 돌안 알파고 마스터 수준, 2017
* 구글 듀플렉스 - 구글 어시스턴트에 들어가 있는 비즈니스용 챗봇을 만들어 주는 인공지능, 스스로 대화 가능

#### AI로 어떤 부분에 응용 되고 어떤걸 할 수 있나?

* Visual Intelligence
  * NMIST 숫자 필기 인식
  * ImageNet 이미지 인식- transfer learning, 허스키로 배운 것으로 진돗개 분류
* Language Intelligence
  * SQUAD Dataset Q&A
  * Machine Translation
    * 최근에는 Parallel Corpus
    * Europarl Corpus
    * UN Parallel Corpus
    * GLUE Benchmark, 문장을 읽고 알 수 있는 것, 주장하는 것을 알 수 있다

### Linear Regression

* 지도학습에 해당 분류 넘어 regression 주어진 x대해 y를 예상
* 데이터 주어졌을때 원하는 결과값이 분류가 아닌 값으로 내주는 것
* 가격 예측, 소비 패턴 분석 등, 가장 간단한 방법
* 감이 아닌 계산으로 수식이 적합한지 증명해야 한다.
* 선형이라고 하지만 비선형도 가능하다.
  * polynomial Regression
  * Multivariate linear regression
* Maximum likelihood estimation
* Residual Sum of Squares
  * log likelihood
  * Nagative log likelihood(NLL)
  * To minimize NLL, we miniize term
  * 예측 y와 실제 y로 차이 줄이는 W0를 찾는 것
* Reidge Regression
  * 오버 피팅 되는 경우에 regularization 복잡xㅡ 심플O
  * 너무 크거나 작은 값에 페널티를 준다
* regularization effects of big data
  *  L2 norm이나 L2 reg 로 하는데 한계가 있다.
  * 데이터가 많아지면 자연스럽게 튀는 값의 영향이 줄어든다.
  * 트레이닝 데이터를 못 맞춘다면 모델 자체가 잘못 된 것

### Naive Bayes Classifier

* 모형이 말이 되는 모형, 사람처럼 직접적 접근
* 많은 머신 러닝에서 가장 기본이 되는 방법, 결과가 좋지는 않지만 베이스 라인, 오래 됨

#### 분류 문제

* Feature(성질, 데이터 디테일)을 잡아서 바꿔준다.
* image, documentation 등
* 확률 통계로 결혼했을 확률을 뽑는다.
* Digit Recognizer
  * 각 픽셀의 색상 확인, 데이터를 통해 모양을 잡는다.
  * 간단한 버전, i,j에 어떤 식인지 본다.
  * prior 클라스에 대한 확률, 1일 확률이 높다 3일 확률이 높다 등
  * uniform인데 이게 아닌 경우도 있다. 
  * 답은 데이터에서 나온다.
* Parameter Estimation
  * 전문가에게 특정 상황에 물어서 파라미터의 확률을 알아 오는 경우도 있다.
* Naive Bayes for Text
  *  NLP에서 다시 하겠지만 중요한 것중 하나는 빈도수, 문맥이 중요하다.

학습한 것에 안좋은 데이터가 하나 나왔을대 전부 무너질 수 있다.

* Estimation Smoothing
  * 오버피팅 방지
  * 몇 번이나 나왔나?+1, 한번은 나왔다고 하자, 그러면 0은 안나온다.
  * Laplace smoothing
* Tuning on Held-Out Data
  * 람다라는 하이퍼 파라미터
  * training, test, held-out data가 각각있다.

#### Baseline

* NB-Baseline
* baseline을 쓴다고 했는데 최근에는 더 강한 baseline을 쓴다.
* What to do about Errors
  * 다양한 것을 한번에 할 수 있는데 나이브 모델
* Features
  * 일반적인 것에서는 feature는 확률 아닌 무언가의 값
  * 도메인 지식이 중요하다, 분류와 모델

### Neural Network 및 NLP 이해하기

#### Deep Natural Language Processing

word, sentence, and document embedding 그중 워드 임베딩

##### word embedding

* 단어를 벡터로 표현
* Bag of words 에서 연장, 단어의 위치 말고 단어의 빈도만 체크
* word vector representation 벡터로 표현하는 것
* 벡터 비슷하면 비슷한 단어라고 본다.

##### disrtibuted representations

* 신경망을 구현하게 되면 각 신경망마다 다른 컨셉으로 받는다.
* 각각의 뉴런이 인증하는게 달라지면서 다른 결과, 벡터를 나타낸다.
* 50차원 단위로 넘어간다

##### Distributional hypothesis

* 문맥을 보고 뜻을 추측한다.

* 비슷한 문맥에 나왔다면 비슷한 의미라고 볼 수 있을 것이다.

* 유사한 단어들 벡터 공간상에서 유사한 곳에 넣는다.

* word2vec이 유명

  두가지 학습 방법

  * CBOW
    * 문맥 단어가 주어지고 중간에 뭔가?
  * Skip-Gram
    * 중간의 단어가 주어지고 주변에 어떤 단어가 등장 할까?

* Evaluation:word similarity task

  * 단어의 비슷한 정도를 계산한다.

* Evaluation:word Analogy task

  * 단어간의 관계가 어떤 관계인지 계산한다.

* Problems of word-level approach

  * 본적 없는 단어, 형태가 풍부한 단어, 각기 다른 단어로 이해
  * 단어의 구성성
  * 희귀한 단어, 학습 량이 적어

* subword information skip-gram

  * 단어를 형태소, 접두어, 접미어를 전부 찢어서 분석한다.

##### 2018 NLP Trend:Transfer Learning from language models

Elmo,Bert

* 컨텍스트가 없었던 문제
  * 다른 뜻을 가진 것을 같게 보던 것
  * NLP에서 해결
* LST라는 신경망 사용 모델도 있다.
  * input->hidden->output
  * 이전의 정보가 남아 있어서 문맥을 읽을 수 있게 되는 것
  * 가까운 단어만 한다는 한계, 제일 가까운 단어가 제일 관계 있는건 아닐 수도 있다. 길면 학습 힘들어
* ELMO의 포인트
  * 컨텍스트를 구분하는 모델을 잘 구현 한 것
  * Transformers
    * 단어들이 각각 input 시퀀스의 다른 단어들과 연관이 있을 수 있는데 얼마나 다른지는 때마다 다르다.
* Bert의 포인트
  * 앞에 단어만 중요해? 뒤에 단어도 중요해!
  * 뒤에 단어를 이용해서 앞쪽에 영향을 준다.
  * 바이디렉션
  * 포지션을 알아낸다.
  * 두개의 문장이 어떤 관계인지 확인한다 등

