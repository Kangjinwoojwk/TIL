# 20190625

# Big Data

## Clustering

* 다양한 파라미터를 보고 유사도에 따라 K개의 그룹으로 유저들을 나누는 것
* 추천 시스템에서 가장 중요한 부분
* 백화점 고객 구매 상품, 유전자, 텍스트 주제, Facebook 유사 이미지, 콜센터 등
* 다양한 종류의 클러스터 가능
* 포인트 부분집합 갯수만큼 집합 가능, 데이터 많아지면 여러 개 해보기 힘들어, 분류만 다른거기에 같은 집합 통째로 바뀌는건 똑같은 집합이라는 걸 알아야 한다.
* 클러스터별 센터(평균점)과의 거리의 제곱을 더한 값이 작을수록 좋은 클러스터

### 파티셔널 알고리즘

#### K-Means Clustering

* K개의 평균점이라는 뜻
* 임의의 그룹-> 평균점(가상점) -> 점별로 가까운 평균점이 같은 것 끼리 묶는다-> 다시 평균점, 이전 센터와 같으면 stop
* 처음 어떻게 랜덤하게 하느냐에 따라 속도, 결과 다르다. 랜덤을 몇 번 거쳐서 다시 해서 좋은 것을 고른다.

단점

* 클러스터가 크거나 작으면 잘 못찾는다.
* 평균점 부터 공모양만 찾을 수 있다.
* 특정 outlier에 평균점이 끌려가서 오판 되기도 한다.->K-medoids, 실제 있는 것만 쓴다. 실제 포인트로 만든다.

#### Hierarchical Clustering

* bottom-up 방식으로 많이 쓰인다.
* 모든 포인트 독립적, 모든 쌍을 만들어 거리가 가장 가까운 점을 묶어서 클러스터를 만든다. 계속 거리가 가장 가까운걸 합친다. 최종적으로 K개의 클러스터가 될때까지 합친다.
* 머지 하면 포인트가 많아진다. 다수 포인트와의 거리 계산 방법 따라 성능 달라, 클러스터의 평균점 or 모든 페어거리의 평균 or 모든 페어중 가장 큰것 or 모든 페어거리 합or 가장 작은 것
* 각각 어떤 알고리즘을 쓰느냐에 따라 크게 달라져

### Density-based clusterring

* 96년 첫 등장, 뒤에 많은 배리에이션, 어떤 결과를 만드는가?

#### DBSCAN Clustrring

* 어떤 결과를 만드는 가?
* 입실론, 미니멈 넘버 포인트라는 파라미터가 있다. neighbourhood
* directly : 점 p로 부터 입실론 거리안에 있는 집합 Neps(p), 네이버들의 집합, 코어포인트 넘버 따져서 number of point가 minimum이상이면 코어 포인트 아니면 보더 포인트, 한번에 감
* Density-reachable: 몇 번에 걸쳐서든 minimum 보고 계속 따져서 갈 슈 있으면 덴서치 리처블
* Density-connected: 특정 점으로 부터 P로도 q로도 desity-reachable할때
* Density-reachable하다면 같은 클래스로 속한다고 본다. 같은 클러스터 안에 두개가 density-connected하다면 연결 된것 그외는 outlier

### EM clustering

* 가정: 모델이 랜덤하게 만들어져 있다

* 데이터를 가지고 그것이 나온 주머니를 추측 하는 것

* 몇 대 몇으로 선택하고 확인 되는가?

* 어떤 주머니에서 어떤 확률로 주어진 데이터가 만들어 질 수 있는가?

* 그렇다면 주머니의 가능성은 어느게 높은가? 가장 높은게 likelihood

* likelihood를 제일 크게해서 미분해서 0으로 놓으면 식으로 만들어 놓을 수 있다.

* 랜덤하게 잡고 싸이클 돌리다가 비슷해지면 그 모델을 갖는다.

* 다수의 확률 디스크립션을 만든다.

  ![캡처](C:\Users\multicampus\TIL\2학기\big_data\캡처.PNG)

* 각 봉우리가 선택될 확률, 그 확률 따라 주머니가 랜덤으로 출현한다.

* 봉우리, 점이 하나씩 선택 된다고 본다. 어디서 선택 되는지 모르니까 거기서 부터 생성확률을 찾아간다. 최종적으로 모든 확률 보면 1이 되야 한다.

* 어느 봉우리에서 만들어 졌는지 모르니 확률을 전부 더해야 한다.

* 어느 클러스터에 속할지 확률을 계산 후 가장 확률 큰 클러스터에 속한다고 가정한다. 

* 전부 곱해지기 때문에 log를 취해서 구한다.

* Maximize한 것과 그것을 라그랑쥬식한 것을 찾는다.

* 파이(봉우리가 선택될 확률), 뮤(봉우리의 평균점), 시그마(표준편차)를 찾아야 한다. 

* 식들은 이미 다 유도 되어 있다. 알아서 찾아 볼 것

* 가장 제대로 클러스터링이 된다.

### Probabilistic Latent Semantic Indexing(PLSI)

* 문서를 쓰는 걸로 모델링, 주제 별로 클러스터링
* 주제 선호 확률로 주제 선택->랜덤으로 하나 선택-> 주제아래 단어 확률 ->반복
* 주제 아래 주제의 확률- 단어의 확률을 데이터를 보고 찾겠다.
* 단어는 독립적
* E-M을 서로 번갈아 가며 하면서 수렴 시킨다. 초기는 랜덤으로 설정

#### Twitobi

* 팔로워 만으로 추적 못한다.
* 유저별 주제에서 선택할 확률 중요하다.
* 다 곱한 것의 log 함수로 ~~~

#### Recommendation systems

* content based filtering
  * 아이템 입장, 사용자 입장에서 유사한 물건을 찾아서 추천
* collaborative filtering
  * 각 유저는 비슷한 다른 유저와 동일하게 행동한다는 가정- 다른 유저 추천이 영향
  * 직접 점수를 매긴 item에 대한 rating을 이용해 추진
  * 메모리 베이스
  * 모델 베이스



## 실습

* 영상에서 나오는 이론들은 굉장히 오래 된 것, 인터넷 찾아보면 더 나와
* 성공 스토리- 악세서리 가게, 넥플릭스, spotify 등
* weka : data mining tool, 자바기반 프레임 워크
* anaconda: python 패키지 수학, 과학 연산 패키지

### weka

* explorer오픈 open하면 데이터의 통계를 보여준다.
* 미니멈, 맥시멈, 평균, 어디에 얼마나 분포 되어 있는가

### anaconda

* 파이썬 쉽게 이용 지원
* 수학, 과학 패키지, 주피터 노트북

### 프로젝트

영화 추천 알고리즘 PLSI알고리즘 쓰게 될 것

